\documentclass{article}

\usepackage[margin=1in]{geometry}
% \usepackage{newpxtext, newpxmath}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage[colorlinks, linkcolor=cyan]{hyperref}

\usepackage{fancyhdr}
\pagestyle{fancy}
\fancyhf{}
\renewcommand{\headrulewidth}{0pt}
\lhead{CS 282br {$\cdot$} Linear Bandits}
\rhead{8 October 2021}
\rfoot{Page \thepage}


\newtheorem{theorem}{Theorem}[section]
\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]
\newtheorem{algorithm}{Algorithm}[section]


\setlength\parindent{0em}

\begin{document}%
  \vspace*{2em}
  \begin{center}
    {\huge\bfseries Linear Bandits}\vskip1em\par
    Matthew Nazari \hspace{1em} Moni Radev\par
    \{\texttt{\href{mailto:matthewnazari@college.harvard.edu}{matthewnazari},\href{mailto:sradev@college.harvard.edu}{sradev}}\}%
    \texttt{@college.harvard.edu}\vskip5em\par
  \end{center}
  
  \section{Online Learning with Experts} % Motivations from Chapter 7 and Chapter 5 stuff
  \begin{definition}[full feedback]
    In this context, all costs are revealed at the end of every round $t$.
  \end{definition}

  \begin{definition}[experts]
    In a full feedback setting, instead of $K$ arms each corresponding to an action, we have $K$ experts who each predict one of $L$ labels. In the case of binary prediction, expert $e_i$ recommends a binary label: $z_{i,t} \in \{0, 1\}$.
  \end{definition}

  For a problem with $K$ experts and $T$ rounds, consider the cost table $(\, c_t(a) : a \in [K], t \in [T] \,)$. Imagine that costs are decided by some adversary, there are three types of costs:
  \begin{itemize}
    \item \textbf{Deterministic, oblivious adversary}: The cost table is chosen and fixed before round 1. The adversary chooses costs independent of our actions. Here, \[ \textrm{Regret}(T) = \textrm{cost}(ALG) - \min_{a \in [K]} \textrm{cost}(a) \footnote{$\textrm{cost}(a) := \sum_{t \in [T]} c_t(a)$}. \]
    \item \textbf{Randomized oblivious adversary}: The cost table is drawn from a random distribution of cost tables before round 1. If we measure the best arm \emph{in foresight} instead of \emph{in hindsight}, we get \[ \textrm{Regret}(T) = \textrm{cost}(ALG) - \min_{a \in [K]} \mathbb{E}[ \textrm{cost}(a) ]. \]
    \item \textbf{Adaptive adversary}: Costs change depending on the algorithm's past choices. This models scenarios where our choices alter the environment that we operate in. We study regret in terms of the \emph{best-observed arm}, which may not always be satisfactory but is worth studying for specific situations where our actions do not substantially affect the total cost of the best arm.
  \end{itemize}

  \begin{algorithm}[Majority Vote Algorithm]
    Consider binary prediction with experts advice. In each round $t$, pick the aaction chosen by the majority of the experts who did not err in the past.
  \end{algorithm}

  \begin{theorem}
    Assuming a perfect expert, the Majority Vote Algorithm takes at most $\log_2 K$ mistakes.
  \end{theorem}

  Instead of losing trust in an expert completely after one mistake, simply downweight our confidence by some factor.

  \begin{algorithm}[Weighted Majority Algorithm, WMA]
    Given a parameter $\epsilon \in [0, 1]$, initialize confidence weights $w_{a,1} = 1$ for all experts $a$. Make prediction $z_t \in [L]$ using weighted majority vote. Update weights for incorrect experts as follows: $w_{a,t+1} \leftarrow w_{a,t}(1 - \epsilon)$
  \end{algorithm}

  \begin{theorem}
    The number of mistakes WMA makes with $\epsilon \in (0, 1)$ is at most \[ \frac{2}{1 - \epsilon} \textrm{\emph{cost}}^* + \frac{2}{\epsilon} \log K. \]
  \end{theorem}

  However, any deterministic algorithm has total cost $T$ for some deterministic, oblivious adversary. The adversary knows and can rig costs to hurt the algorithm. Therefore, we define a randomized algorithm.
  
  \begin{algorithm}[Hedge Algorithm]
    Given a parameter $\epsilon \in (0, \frac{1}{2})$, initialize confidence weights as in WMA. At each round $t$ sample an arm from $p_t(a)$ where \[ p_t(a) := \frac{w_{a, t}}{\sum_{a'=1}^{K} w_{a', t}}. \] Observe the cost $c_t(a) \in \{0, 1\}$ and update each arm's weight $w_{a,t+1} \leftarrow w_{a,t}(1 - \epsilon)^{c_t(a)}$.
  \end{algorithm}
    
  \section{Online Routing Problem}  
  In ``linear bandits,'' an action is a low-dimensional vector and the costs that round are linear. The cost at any round $t$ for an action $a \in \mathbb R^d$ is $c_t(a) := a \cdot v_t$ where $v_t \in \mathbb R^d$ differs per round but not per arm. \vskip1em

  In full feedback setting, use Hedge Algorithm with parameter $\epsilon = 1/\sqrt{dT}$ to achieve regret $\mathbb E[ \text{Regret}(T)] \leq O(d \sqrt{dT})$. \vskip1em

  In this problem, different feedbacks are
  \begin{itemize}
    \item \textbf{bandit feedback}: only cost $c_t(a_t)$ is observed;
    \item \textbf{semi-bandit feedback}: costs $c_t(e)$ for all $e \in a_t$ are observed;
    \item \textbf{full feedback}: costs $c_t(e)$ for all edges are observed.
  \end{itemize}

  Reduction to the Bandit Problem: Idea is to use the Hedge algorithm. This requires us to determine two things: a \emph{selection rule} for using expert $e_t$ to ick arm $a_t$, and defining "fake costs" $\widehat{c}_t(e)$ for all experts.

  Explore by choosing edge $e$ uniformly at random. 
  \begin{algorithm}[Semi-Bandit Hedge Algorithm]
    With our reduction from the Hedge Algorithm to the bandit problem, select an edge $e$ randomly at uniform with probability $\gamma$. Choose a path that includes this edge and define fake costs as follows: \[ \widehat{c}_{t}(e)= \begin{cases}\frac{c_{t}(e)}{\gamma / d} & \text { if event } \Lambda_{t, e} \text { happens } \\ 0 & \text { otherwise }\end{cases} \] where $\Lambda_{t, e}$ is the event that in round $t$, we choose random exploration and edge $e$ to explore.
  \end{algorithm}

  We want fake costs to be unbiased estimates of true costs, so that \[ \mathbb{E} \left[ \widehat{\textrm{Regret}}_\text{Hedge} \right] \geq \mathbb{E} \left[ \textrm{Regret}_\text{Hedge}(T)  \right]. \]

  \section{Combinatorial Semi-Bandits}
  Replace edges with $d$ ``atoms,'' and $u-v$ paths with feasible subsets $a \in \mathcal F$ of atoms $S$.

  Notable special cases include
  \begin{itemize}
    \item \emph{news articles}: a site can only select a subset of articles to display. A user can either click or ignore on each article, and feasible subsets represent various constraints on articles.
    \item \emph{advertisements}: a website can only display a subset of ads to each user, and the website recieves payment if the user clicks and even which type of user clicks. 
  \end{itemize}

  The largest challenge of solving bandit feedback version is estimating fake costs for all atoms in the chosen action. 

  \section{Follow Perturbed Leader}
  Back to the full-feedback setting where we have any fixed arbitrary subset $\mathcal A \subset [0, 1]^d$ of feasible actions. We incur a cost $c_t(a) = v_t \cdot a$ and observe $v_t$ after every round.

  \begin{definition}[optimization oracle]
    A subroutine which computes the best action for a given cost vector: $M(v) \in \textrm{argmin}_{a \in \mathcal A} \; a \cdot v$.
  \end{definition}

  \begin{algorithm}[Follow the Leader]
    Choose the action $a_{t+1} = M(v_{1:t})$  where $v_{i:j} = \sum_{t=i}^j v_t \in \mathbb R^d$.
  \end{algorithm}

  Follow the Leader breaks when adversarial costs can synchronize its costs to harm the algorithm: the total cost for any deterministic algorithm is again T. Consider

  \[
  \begin{align*}
    \mathcal A &= \{ (1, 0), (0, 1) \} \\
    v_1 &= (\frac{1}{3}, \frac{2}{3}) \\
    v_t &= \begin{cases}
      (1, 0) & \text{if $t$ is even}, \\
      (0, 1) & \text{if $t$ is odd}.
    \end{cases}
  \end{align*}  
  \]

  Then
  \[
    v_{1:t} = \begin{cases}
      (i + \frac{1}{3}, i - \frac{1}{3}) & \text{if $t = 2i$}, \\
      (i + \frac{1}{3}, i + \frac{2}{3}) & \text{if $t = 2i + 1$}.
    \end{cases}
  \]

  \begin{algorithm}[Follow the Perturbed Leader, FPL]
    Pretend there was a 0-th round, with $v_0 \in \mathbb R^d \sim \mathcal D$.
  \end{algorithm}

  \begin{theorem}
    Assume $v_t \in [0, U/d]^d$ for some known parameter $U$. Then FPL achieves regret \[ \mathbb E [\textrm{Regret}(T)] \leq 2U\sqrt{dT} \] with running time in each round polynomial in $d$ plus one call to the oracle.
  \end{theorem}

\end{document}
