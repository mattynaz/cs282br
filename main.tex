\documentclass{article}

\usepackage[margin=1in]{geometry}
% \usepackage{newpxtext, newpxmath}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage[colorlinks, linkcolor=cyan]{hyperref}
\usepackage{titlesec}
\usepackage{fancyhdr}
\pagestyle{fancy}
\fancyhf{}
\renewcommand{\headrulewidth}{0pt}
\lhead{CS 282br {$\cdot$} Linear Bandits}
\rhead{8 October 2021}
\rfoot{Page \thepage}


\newtheorem{theorem}{Theorem}
\theoremstyle{definition}
\newtheorem{definition}{Definition}
\newtheorem{algorithm}{Algorithm}


\setlength\parindent{0em}

\begin{document}%
  \vspace*{3em}
  \centerline{\huge\bfseries Linear Bandits}
  \begin{center}
    Matthew Nazari \hspace{1em} Moni Radev\par
    \texttt{%
    \{%
    \href{mailto:matthewnazari@college.harvard.edu}{matthewnazari},%
    \href{mailto:sradev@college.harvard.edu}{sradev}%
    \}%
    @college.harvard.edu}\vspace{5em}\par
  \end{center}
  
  \section{Online Learning with Experts} % Motivations from Chapter 7 and Chapter 5 stuff
  \begin{definition}[full-feedback]
    In this context, all costs are revealed at the end of every round $t$.
  \end{definition}

  \begin{definition}[experts]
    In a full-feedback setting, instead of $K$ arms each corresponding to an action, we have $K$ experts who each predict one of $L$ labels. In the case of binary prediction, expert $e_i$ recommends a binary label: $z_{i,t} \in \{0, 1\}$.
  \end{definition}

  For a problem with $K$ experts and $T$ rounds, consider the cost table $(\, c_t(a) : a \in [K], t \in [T] \,)$. Imagine that costs are decided by some adversary, there are three types of costs:
  \begin{itemize}
    \item \textbf{Deterministic, oblivious adversary}: The cost table is chosen and fixed before round 1. The adversary chooses costs independent of our actions. Here, \[ \text{Regret}(T) = \text{cost}(ALG) - \min_{a \in [K]} \text{cost}(a) \footnote{$\text{cost}(a) := \sum_{t \in [T]} c_t(a)$}. \]
    \item \textbf{Randomized oblivious adversary}: The cost table is drawn from a random distribution of cost tables before round 1. If we measure the best arm \emph{in foresight} instead of \emph{in hindsight}, we get \[ \text{Regret}(T) = \text{cost}(ALG) - \min_{a \in [K]} \mathbb{E}[ \text{cost}(a) ]. \]
    \item \textbf{Adaptive adversary}: Costs change depending on the algorithm's past choices. This models scenarios where our choices alter the environment that we operate in. We study regret in terms of the \emph{best-observed arm}, which may not always be satisfactory but is worth studying for specific situations where our actions do not substantially affect the total cost of the best arm.
  \end{itemize}

  \begin{algorithm}[Majority Vote Algorithm]
    Consider binary prediction with experts advice. In each round $t$, pick the aaction chosen by the majority of the experts who did not err in the past.
  \end{algorithm}

  \begin{theorem}
    Assuming a perfect expert, the Majority Vote Algorithm takes at most $\log_2 K$ mistakes.
  \end{theorem}

  Instead of losing trust in an expert completely after one mistake, simply downweight our confidence by some factor.

  \begin{algorithm}[Weighted Majority Algorithm, WMA]
    Given a parameter $\epsilon \in [0, 1]$, initialize confidence weights $w_{a,1} = 1$ for all experts $a$. Make prediction $z_t \in [L]$ using weighted majority vote. Update weights for incorrect experts as follows: $w_{a,t+1} \leftarrow w_{a,t}(1 - \epsilon)$
  \end{algorithm}

  \begin{theorem}
    The number of mistakes WMA makes with $\epsilon \in (0, 1)$ is at most \[ \frac{2}{1 - \epsilon} \text{\emph{cost}}^* + \frac{2}{\epsilon} \log K. \]
  \end{theorem}

  However, any deterministic algorithm has total cost $T$ for some deterministic, oblivious adversary. The adversary knows and can rig costs to hurt the algorithm. Therefore, we define a randomized algorithm.
  
  \begin{algorithm}[Hedge Algorithm]
    Given a parameter $\epsilon \in (0, \frac{1}{2})$, initialize confidence weights as in WMA. At each round $t$ sample an arm from $p_t(a)$ where \[ p_t(a) := \frac{w_{a, t}}{\sum_{a'=1}^{K} w_{a', t}}. \] Observe the cost $c_t(a) \in \{0, 1\}$ and update each arm's weight $w_{a,t+1} \leftarrow w_{a,t}(1 - \epsilon)^{c_t(a)}$.
  \end{algorithm}
    
  \section*{Reduction to the Bandit Problem} % Chapter 6 content
  Idea is to use the Hedge algorithm. This requires us to determine two things: a \emph{selection rule} for using expert $e_t$ to ick arm $a_t$, and defining "fake costs" $\widehat{c_t}(e)$ for all experts.

  \section{Online Routing Problem}

  \section{Combinatorial Semi-Bandits}

  \section{Follow Perturbed Leader}

  \section{Literature Review}

\end{document}




% \documentclass[centertitle]{notes}
% \usepackage{lipsum}

% \coursetitle{An Introduction to Advanced Topics}
% \subtitle{Lecture Notes}
% \coursecode{am224}
% \professor{Khabib Nurmagomedov\thanks{Let's talk now.}}
% \scribe{
%   Matthew Nazari\titlehref[mailto:matthewnazari@college.harvard.edu]{matthewnazari@college}[email]
%   \and
%   Bella Tarantino\titlehref[mailto:bellatarantino@college.harvard.edu]{bellatarantino@college}[email]
% }
% \place{Harvard University}
% \flag{This is a example document. Ut lorem lorem, interdum eu, tincidunt sit amet, laoreet vitae, arcu.}
% \date{31}{2}{2024}

% \begin{document}


%   \section{Introduction}
%   \lipsum[1]
%   \marginpar{Ut lorem lorem, interdum eu, tincidunt sit amet, laoreet vitae, arcu. Aenean faucibus pede eu ante.}
%   \lipsum[2]
%   \subsection{This is a subsection}
%   \lipsum[3]
%   \begin{marginfig}[Nunc elementum fermentum wisi. Aenean placerat\protect\footnotemark]
%     \begin{tikzpicture}[main/.style = {draw, circle}]
%       \node[main] (1) {$x_1$};
%       \node[main] (2) [above right of=1] {$x_2$};
%       \node[main] (3) [below right of=1] {$x_3$}; 
%       \node[main] (4) [above right of=3] {$x_4$};
%     \end{tikzpicture}
%   \end{marginfig}
%   \footnotetext{Donec eu purus.Quisque vehicula, urna sed ultricies auctor.}
%   \lipsum[4-5]
%   \begin{definition}[statistical model]
%     A {\italics statistical model} is a family of probability distributions indexed by a parameter $\theta \in \Theta$ where $\Theta$, the {\italics parameter space}, is the set of all allowable parameter values.
%   \end{definition}
%   \lipsum[6]
%   \marginpar{Suspendisse vel felis. Ut lorem lorem, interdum eu, tincidunt sit amet, laoreet vitae, arcu.Aenean faucibus pede eu ante.}
%   \subsection{Still introducting things}
%   \lipsum[7]


%   \section{A continuation}
%   \lipsum[8]
%   \begin{proposition}
%     It is given that if $\E[\theta]$ exists, then $\MSE(\hat\theta) = \E[\hat\theta - \theta]^2$.
%   \end{proposition}
%   \begin{theorem}[Theorem]
%     The mean square error of an estimator $\hat\theta$ is the variance plus the square of the bias: $$\MSE(\hat\theta) = \Var(\hat\theta) + (\bias(\hat\theta))^2.$$
%   \end{theorem}
%   \begin{proof} This theorem directly follows from the definition of variance:
%     \begin{align*}
%       \MSE(\hat\theta) &= \E[\hat\theta - \theta]^2 \\
%                        &= \Var(\hat\theta - \theta) + (\E[\hat\theta - \theta])^2 \\
%                        &= \Var(\hat\theta - \theta) + (\E[\hat\theta] - \theta)^2 \\
%                        &= \Var(\hat\theta) + (\bias(\hat\theta))^2.
%     \end{align*}
%   \end{proof}
%   \marginpar[2em]{\it{Caution.} Nulla facilisi. Pellentesque eget lectus. Proin eu metus. Sed porttitor.}
%   \lipsum[9]\footnote{Nulla facilisi. Pellentesque eget lectus. Proin eu metus. Sed porttitor.}
%   \begin{example}[This is an example]
%     The family of all normal distributions, $\{\N(\mu, \sigma^2) : \mu \in \R, \sigma > 0\}$, is a 2-dimensional parametric model where $\Theta = \{\theta : \theta \in \R \times \R^+\}$. But models don't have to be named distributions. Suppose we have a population of cats and dogs, and we are studying the weight $Y$ of a random animal from the population. Let $p$ be the proportion of cats in the population. Suppose the weight of a random cat and a random dog is $\N(\mu_1, {\sigma_1}^2)$ and $\N(\mu_2, {\sigma_2}^2)$ respectively. Then by the LOTP, the CDF of $Y$ is 
%     \begin{align*}
%       F_Y(y) = P(Y \leq y) &= P(Y \leq y \mid \t{cat})P(\t{cat}) + P(Y \leq y \mid \t{dog})P(\t{dog}) \\
%                           &= p\,\Phi(\frac{y - \mu_1}{\sigma_1}) + (1-p)\,\Phi(\frac{y - \mu_2}{\sigma_2}).
%     \end{align*}
%     The CDF of $Y$ depends on the parameter $\theta = (p, \mu_1, \sigma_1, \mu_2, \sigma_2)$, so we write it as as $F_Y(y \mid \theta)$. Our model, therefore, is the collection of all CDFs $F_Y(y \mid \theta)$ indexed by $\theta \in \Theta =\{\theta : \theta \in [0, 1] \times \R \times \R^+ \times \R \times \R^+\}$. This is a 5-dimensional parametric model.
%   \end{example}
%   \lipsum[10]
%   \begin{note}
%     liquam vestibulum fringilla lorem. Sed neque lectus, consectetuerat, consectetuer sed, eleifend ac, lectus. Nulla facilisi. Pellentesque eget lectus. Proin eu metus.Sed porttitor. In hac habitasse platea dictumst. Suspendisse eu lectus. Ut mi mi, lacinia sit amet, placerat et.
%   \end{note}
%   \lipsum[11]
%   \begin{fig}[Ut lorem lorem, interdum eu, tincidunt sit amet, laoreet vitae, arcu. Aenean faucibus pede eu ante. Amet, laoreet vitae, arcu.]%
%     \begin{tikzpicture}[main/.style = {draw, circle}]
%       \node[main] (1) {$x_1$};
%       \node[main] (2) [above right of=1] {$x_2$};
%       \node[main] (3) [below right of=1] {$x_3$}; 
%       \node[main] (4) [above right of=3] {$x_4$};
%     \end{tikzpicture}
%   \end{fig}
%   \lipsum[12-13]
%   \subsection{Planets, moons, and stars}
%   \lipsum[14]
%   \begin{remark}
%     Etiam euismod. Fusce facilisis lacinia dui. Suspendisse potenti. In mi erat, cursus id, nonummy sed, ullamcorper eget, sapien.
%   \end{remark}
%   \lipsum[14]


%   \section{A Final Section}
%   \lipsum[15]
%   \subsection{Closing Remarks}
%   \lipsum[16]

  
% \end{document}
